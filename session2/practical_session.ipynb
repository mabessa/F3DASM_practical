{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session demo #2 (students)\n",
    "\n",
    "In this session, we are covering the following:\n",
    "* Implement your own sampling strategy\n",
    "* Implement your own optimizer\n",
    "* How to add your implementations to the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are downloading some objects if you might not finish the exercise in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -cO - https://github.com/mpvanderschelling/F3DASM_practical/blob/main/session2/exercise_gridsearch_samples.obj?raw=true > exercise_gridsearch_samples.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are importing the f3dasm package. If this is not present, install it from the PyPi index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 14:49:31.330698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 14:49:31.499204: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-28 14:49:32.215315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64\n",
      "2022-11-28 14:49:32.215408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64\n",
      "2022-11-28 14:49:32.215415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# If f3dasm is not found, install the correct version from pip\n",
    "try:\n",
    "    import f3dasm\n",
    "except ModuleNotFoundError:\n",
    "    %pip install f3dasm==0.2.6 --quiet\n",
    "    import f3dasm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we have to make sure that we have the right version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you got the right version, otherwise uninstall this version!\n",
    "\n",
    "import os\n",
    "\n",
    "if f3dasm.__version__ != '0.2.6':\n",
    "    %pip uninstall -y f3dasm\n",
    "    os._exit(00)\n",
    "    # The kernel will be restarted and you need to run the notebook again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your f3dasm version is 0.2.6!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Your f3dasm version is {f3dasm.__version__}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import some other packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify a seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own sampler\n",
    "\n",
    "In this part of the tutorial, we are going to take a closer look at the `f3dasm.SamplingInterface`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens inside the samples\n",
    "\n",
    "By executing the `get_samples()` function, the following happesn\n",
    "\n",
    "* We split the parameters in the designspace to each of the categories (`ContinuousParameter`, `DiscreteParameter`, `CategoricalParameter`, `ConstantParameter`)\n",
    "* Samples from the discrete and categorical dimensions are by default randomly sampled (`_sample_discrete()` and `_sample_categorical()`)\n",
    "* Samples from the constant parameter are, well .. constant!\n",
    "* Samples from the continuous parameters get samples by the samplingstrategy implemente by the `sample_continuous()` function\n",
    "* All samples are concatenated to a numpy array\n",
    "* The samples are converted to a `f3dasm.Data` object!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a new samples\n",
    "\n",
    "Implementing a new sampler goes as follows\n",
    "\n",
    "* We create a new class inhereting from the `f3dasm.SamplingInterface` class\n",
    "* We have to implement our own `sample_continuous()` function:\n",
    "\n",
    "> Note we can also implement sampling strategies for all the other parameters but this is not necessary\n",
    "\n",
    "This `sample_continuous` function inputs the number of samples you want to create and returns a 2D numpy-array with the coordinates of those samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSampler(f3dasm.SamplingInterface):\n",
    "    def sample_continuous(self, numsamples: int) -> np.ndarray:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: GridSearch\n",
    "\n",
    "We are going to create a new continuous sampling strategy.\n",
    "\n",
    "The idea of grid search, or parameter sweep, is to equally partition the design space to a grid and sample on these grid points.\n",
    "\n",
    "The proposed strategy is as follows:\n",
    "* We will sample a $n$-dimensional grid of points according to the number of samples requested\n",
    "* If we have sample budget left, these points are randomly sampled and added to the gridpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Grid Search example](https://upload.wikimedia.org/wikipedia/commons/b/b6/Hyperparameter_Optimization_using_Grid_Search.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll help you through the mathematics so that you have to implement it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a unit 2D grid and we want to sample 18 points according to this grid search strategy, we will have a grid of 4x4 and 2 points left.\n",
    "The points per dimension is therefore $4$. We can calculate this `points_per_dimension` by taking the $n^{\\mathrm{th}}$-root of `numsamples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points per dimension is: 4\n"
     ]
    }
   ],
   "source": [
    "numsamples = 18\n",
    "dimensions = 2\n",
    "\n",
    "points_per_dimension = int(np.power(numsamples, 1/dimensions))\n",
    "print(f\"Points per dimension is: {points_per_dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of \"leftover samples\" is just substracting all the grid points from the total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leftover samples is: 2\n"
     ]
    }
   ],
   "source": [
    "leftovers = numsamples - points_per_dimension**dimensions\n",
    "print(f\"Leftover samples is: {leftovers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the spacing for one dimension, we create a `spacing_list` denoting the coordinate of the grid points\n",
    "\n",
    "> We will start at the origin, so we substract $1$ from `points_per_dimension`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.3333333333333333, 0.6666666666666666, 1.0]\n"
     ]
    }
   ],
   "source": [
    "spacing_list = [i*(1/(points_per_dimension-1)) for i in range(points_per_dimension)]\n",
    "print(spacing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Be aware that if the number of samples is less or equal than the $dim^2$, you will divide by $0$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create all grid points, we will consult the `product` function from the `itertools` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.         0.33333333]\n",
      " [0.         0.66666667]\n",
      " [0.         1.        ]\n",
      " [0.33333333 0.        ]\n",
      " [0.33333333 0.33333333]\n",
      " [0.33333333 0.66666667]\n",
      " [0.33333333 1.        ]\n",
      " [0.66666667 0.        ]\n",
      " [0.66666667 0.33333333]\n",
      " [0.66666667 0.66666667]\n",
      " [0.66666667 1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.33333333]\n",
      " [1.         0.66666667]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "samples_gridsearch = np.array([item for item in product(spacing_list, repeat=dimensions)])\n",
    "print(samples_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`samples_gridsearch` denotes the samples that are created by the equally spaced grid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Exercise #1**\n",
    "\n",
    "Implement the grid search sampler in the `f3dasm` framework\n",
    "\n",
    "* Create the grid samples and make sure you are not using more points than the `numsamples`\n",
    "* If you have remaining samples, sample them randomly and add them to the total generated samples\n",
    "* If `numsamples` <= $dim^2$, you may raise a `ValueError`\n",
    "* In order to test your implementation, sample $123$ points on a 2D continuous space with bounds $-1.0$ and $1.0$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpful functions:\n",
    "\n",
    "* Remember, a sampler is initialized with a `f3dasm.DesignSpace` and a `seed`\n",
    "* A list of the continuous parameters can be accessed with `self.design.get_continuous_input_parameters()`\n",
    "* You can sample in a unit hypercube and stretch the samples to the boundaries of the designspace with the `self._stretch_samples(samples)` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are not able to finish this exercise on time, you may import the samples by uncommenting the following line:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = f3dasm.read_pickle('exercise_gridsearch_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate your sampler into the framework\n",
    "\n",
    "The next step is to integrate your sampler to the f3dasm code so that others can access them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the sampler to the list of samplers\n",
    "\n",
    "In order to check if your implementation is compatible, you can add it to the list of implemented samplers\n",
    "All the samplers can be accessed in a list by calling `f3dasm.sampling.SAMPLERS` \n",
    "\n",
    "> This list is created in the sub-module initializer`f3dasm.sampling.__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[f3dasm.sampling.latinhypercube.LatinHypercube,\n",
       " f3dasm.sampling.randomuniform.RandomUniform,\n",
       " f3dasm.sampling.sobolsequence.SobolSequence]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3dasm.sampling.SAMPLERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the sampler to the source code\n",
    "\n",
    "You can create a new `.py` file in the `/f3dasm/sampling/` folder in the `f3dasm` repository with the sampler\n",
    "\n",
    "If you have obeyed the guidelines of creating new implementations, you may create a pull request.\n",
    "Your implementaion will be reviewed by the main developers of the framework and if no suggestions are made, your implementation is added to the framework.\n",
    "\n",
    "As of today, no guidelines are yet present, but they will be minimally include:\n",
    "\n",
    "* Having proper tests\n",
    "* Having a numpy-styledocstring\n",
    "* Sphinx documentation with working examples\n",
    "\n",
    "The guidelines can be found [on the wiki page in the GitHub repository](https://github.com/bessagroup/F3DASM/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own optimizer\n",
    "\n",
    "Now we are going to implement a new benchmark function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a class storing the potential hyper-parameters for our optimizers. Even if we our optimizer doesn't have hyper-parameters, you still have to create class\n",
    "\n",
    "This class has to be inhereted from the `f3dasm.OptimizerParameters` class. This inhereted class consists two mandatory attributes: \n",
    "\n",
    "* `population`: how many points are created for each update step. Defaults to 1\n",
    "* `force_bounds`: if the optimizer is forced to stay between the design bounds. Defaults to True. Currently does not work when set to False!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewOptimizer_Parameters(f3dasm.OptimizerParameters):\n",
    " \"\"\"Example of hyperparameters\"\"\"\n",
    "\n",
    " example_hyperparameter_1: float = 0.999\n",
    " example_hyperparameter_2: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an new optimizer by inheriting from the `f3dasm.Optimizer` class\n",
    "\n",
    "* We create a class attribute `parameter` and initialize it without any arguments in order to use the defaults specified above\n",
    "* The only function we have to implement is the `update_step()` function, which takes a `f3dasm.Function` and outputs a tuple containing the position and evaluated value of the next iteration\n",
    "* The `init_parameters()` function is optional. It can be used to store dynamic hyper-parameters that update throughout updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewOptimizer(f3dasm.Optimizer):\n",
    " \"\"\"Example of implement your own optimizer\"\"\"\n",
    "\n",
    " parameter: NewOptimizer_Parameters = NewOptimizer_Parameters()\n",
    "\n",
    " def init_parameters(self):\n",
    "    \"\"\"Set the dynamic initialization parameters. These are resetted every time the iterate method is called.\"\"\"\n",
    "    pass\n",
    "\n",
    " def update_step(self, function: f3dasm.Function) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Custom update step for your own optimizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    function\n",
    "        objective function that is being optimized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tuple of resulting input and output parameter\n",
    "    \"\"\"\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the optimizer, we call the `iterate()` method, which for-loops over the `update_step()` method, appending the `x` and `y` values to the internal data-object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Momentum optimizer\n",
    "\n",
    "As an exercise, we are going to code the momentum optimizer from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Momentum optimizer example](https://miro.medium.com/max/640/1*cZF62SbHrfhQ595O80w3nw.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum update-step can be described as follows:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\lambda \\nabla_x J(x_{t-1}) \\\\\n",
    "x_t = x_{t-1} - v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $v_{t-1}$ the previous velocity (**dynamic**)\n",
    "* $v_t$ the new velocity\n",
    "* $x_{t_1}$ is the previous iteration\n",
    "* $x_t$ the resulting iteration\n",
    "* $\\lambda$ is the learning rate (**static**)\n",
    "* $\\gamma$ is the momentum term (**static**)\n",
    "* $\\nabla_x J(x_{t-1})$ the gradient of the previous iteration (can be calculated by the `dfdx()` method of the `f3dasm.Function`)\n",
    "\n",
    "The velocity is initialized for every dimension $0$. The default parameters of the `learning_rate` ($\\lambda$) and `momentum` ($\\gamma$) are $0.9$ and $1e-4$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Exercise #2**\n",
    "\n",
    "Implement the Momentum optimizer in the `f3dasm` framework\n",
    "\n",
    "* Creat a `Momentum_Exercise_Parameters`-optimizer-parameter-class that has been inhereted from `f3dasm.OptimizerParameters`\n",
    "* Make sure to specify the `momentum=0.9` and `learning_rate=1e-4` parameters here!\n",
    "* Create a `Momentum_Exercise`-optimizer class that has been inhereted from `f3dasm.Optimizer`\n",
    "* Part of the code has already been filled in, regarding selecting the input candidate and data formatting\n",
    "\n",
    "* In order to test your implementation, call the `iterate()` for 100 iterations on a 2D `Levy()` function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "# Partially coded momentum optimizer. Make sure to add on to this!\n",
    "class Momentum_Exercise(f3dasm.Optimizer):\n",
    "   def update_step(self, function: f3dasm.Function) -> Tuple[np.ndarray, np.ndarray]:\n",
    "      x = self.data.get_input_data().iloc[-1].to_numpy()\n",
    "\n",
    "      ...\n",
    "\n",
    "      # Format and force bounds\n",
    "      x_update = np.atleast_2d(self._force_bounds(x_update))\n",
    "      y_update = function(x_update)\n",
    "      return x_update, y_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate your optimizer into the framework\n",
    "\n",
    "The next step is to integrate your optimizer to the f3dasm code so that others can access them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the optimizer to the list of optimizers\n",
    "\n",
    "In order to check if your implementation is compatible, you can add it to the list of implemented samplers\n",
    "All the samplers can be accessed in a list by calling `f3dasm.sampling.OPTIMIZERS` \n",
    "\n",
    "> This list is created in the sub-module initializer`f3dasm.optimizer.__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[f3dasm.optimization.cmaesadam.CMAESAdam,\n",
       " f3dasm.optimization.adam.Adam,\n",
       " f3dasm.optimization.sgd.SGD,\n",
       " f3dasm.optimization.cg.CG,\n",
       " f3dasm.optimization.cmaes.CMAES,\n",
       " f3dasm.optimization.differentialevolution.DifferentialEvolution,\n",
       " f3dasm.optimization.simulatedannealing.SimulatedAnnealing,\n",
       " f3dasm.optimization.lbfgsb.LBFGSB,\n",
       " f3dasm.optimization.neldermead.NelderMead,\n",
       " f3dasm.optimization.pso.PSO,\n",
       " f3dasm.optimization.randomsearch.RandomSearch,\n",
       " f3dasm.optimization.sga.SGA,\n",
       " f3dasm.optimization.sea.SEA,\n",
       " f3dasm.optimization.xnes.XNES,\n",
       " f3dasm.optimization.rmsprop.RMSprop,\n",
       " f3dasm.optimization.nadam.Nadam,\n",
       " f3dasm.optimization.adamax.Adamax,\n",
       " f3dasm.optimization.ftrl.Ftrl,\n",
       " f3dasm.optimization.sade.SADE]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3dasm.optimization.OPTIMIZERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the optimizer to the source code\n",
    "\n",
    "You can create a new `.py` file in the `/f3dasm/optimization/` folder in the `f3dasm` repository with the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the tutorial\n",
    "\n",
    "This was the end of the second tutorial session on the f3dasm framework! Feel free to ask questions and give me feedback about this tutorial or the framework by [creating an issue on GitHub](https://github.com/bessagroup/F3DASM/issues)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('f3dasm_env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f45089fc4d6167869e7370c5b47568ce8eb74326038b740d9fc0c36d2e164509"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
